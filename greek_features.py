# -*- coding: utf-8 -*-

from textual_feature import textual_feature, setup_tokenizers
from functools import reduce
from unicodedata import normalize
#Reference for normalization: https://jktauber.com/articles/python-unicode-ancient-greek/

setup_tokenizers(('.', ';', 'Í¾')) #'FULL STOP', 'SEMICOLON', 'GREEK QUESTION MARK'

@textual_feature('sentence_words', 'ancient_greek')
def freq_interrogatives(file):
	num_interrogative = 0
	interrogative_chars = {';', 'Í¾'}
	for line in file:
		num_interrogative += reduce(lambda cur_count, word: cur_count + 1 if word in interrogative_chars else 0, line, 0)

	return num_interrogative / len(file)

@textual_feature('words', 'ancient_greek')
def freq_conditional_markers(file):
	num_conditional_words = 0
	num_characters = 0
	conditional_words = {'Îµá¼°', 'Îµá¼´', 'Îµá¼²', 'á¼Î¬Î½', 'á¼á½°Î½'}
	conditional_words = conditional_words | \
	{normalize('NFD', val) for val in conditional_words} | \
	{normalize('NFC', val) for val in conditional_words} | \
	{normalize('NFKD', val) for val in conditional_words} | \
	{normalize('NFKC', val) for val in conditional_words}

	for word in file:
		num_conditional_words += 1 if word in conditional_words else 0
		num_characters += len(word)

	return num_conditional_words / num_characters

@textual_feature('words', 'ancient_greek')
def freq_personal_pronouns(file):
	num_pronouns = 0
	num_characters = 0
	personal_pronouns = {'á¼Î³Ï', 'á¼Î³á½¼', 'á¼Î¼Î¿á¿¦', 'Î¼Î¿Ï…', 'á¼Î¼Î¿Î¯', 'á¼Î¼Î¿á½¶', 'Î¼Î¿Î¹', 'á¼Î¼Î­', 'á¼Î¼á½²', 'Î¼Îµ', 'á¼¡Î¼Îµá¿–Ï‚', 'á¼¡Î¼á¿¶Î½', 
	'á¼¡Î¼á¿–Î½', 'á¼¡Î¼á¾¶Ï‚', 'ÏƒÏ', 'Ïƒá½º', 'ÏƒÎ¿á¿¦', 'ÏƒÎ¿Ï…', 'ÏƒÎ¿Î¯', 'ÏƒÎ¿á½¶', 'ÏƒÎ¿Î¹', 'ÏƒÎ­', 'Ïƒá½²', 'ÏƒÎµ', 'á½‘Î¼Îµá¿–Ï‚', 'á½‘Î¼á¿¶Î½', 'á½‘Î¼á¿–Î½', 'á½‘Î¼á¾¶Ï‚', 'Î¼', 'Ïƒ'}
	personal_pronouns = personal_pronouns | \
	{normalize('NFD', val) for val in personal_pronouns} | \
	{normalize('NFC', val) for val in personal_pronouns} | \
	{normalize('NFKD', val) for val in personal_pronouns} | \
	{normalize('NFKC', val) for val in personal_pronouns}

	for word in file:
		num_pronouns += 1 if word in personal_pronouns else 0
		num_characters += len(word)

	return num_pronouns / num_characters

@textual_feature('words', 'ancient_greek')
def freq_demonstrative(file):
	num_demonstratives = 0
	num_characters = 0
	demonstrative_pronouns = {'á¼ÎºÎµá¿–Î½Î¿Ï‚', 'á¼ÎºÎµÎ¯Î½Î¿Ï…', 'á¼ÎºÎµÎ¯Î½á¿³', 'á¼ÎºÎµá¿–Î½Î¿Î½', 'á¼ÎºÎµá¿–Î½Î¿Î¹', 'á¼ÎºÎµÎ¯Î½Ï‰Î½', 'á¼ÎºÎµÎ¯Î½Î¿Î¹Ï‚', 'á¼ÎºÎµÎ¯Î½Î¿Ï…Ï‚', 
	'á¼ÎºÎµÎ¯Î½Î·', 'á¼ÎºÎµÎ¯Î½Î·Ï‚', 'á¼ÎºÎµÎ¯Î½á¿ƒ', 'á¼ÎºÎµÎ¯Î½Î·Î½', 'á¼ÎºÎµá¿–Î½Î±Î¹', 'á¼ÎºÎµÎ¯Î½Î±Î¹Ï‚', 'á¼ÎºÎµÎ¯Î½á¾±Ï‚', 'á¼ÎºÎµÎ¯Î½Î±Ï‚', 'á¼ÎºÎµá¿–Î½Î¿', 'á¼ÎºÎµá¿–Î½Î±', 'á½…Î´Îµ', 
	'Ï„Î¿á¿¦Î´Îµ', 'Ï„á¿·Î´Îµ', 'Ï„ÏŒÎ½Î´Îµ', 'Î¿á¼µÎ´Îµ', 'Ï„á¿¶Î½Î´Îµ', 'Ï„Î¿á¿–ÏƒÎ´Îµ', 'Ï„Î¿ÏÏƒÎ´Îµ', 'á¼¥Î´Îµ', 'Ï„á¿†ÏƒÎ´Îµ', 'Ï„á¿‡Î´Îµ', 'Ï„Î®Î½Î´Îµ', 'Î±á¼µÎ´Îµ', 'Ï„Î±á¿–ÏƒÎ´Îµ', 
	'Ï„á¾±ÌÏƒÎ´Îµ', 'Ï„Î¬ÏƒÎ´Îµ', 'Ï„ÏŒÎ´Îµ', 'Ï„Î¬Î´Îµ', 'Î¿á½—Ï„Î¿Ï‚', 'Ï„Î¿ÏÏ„Î¿Ï…', 'Ï„Î¿ÏÏ„á¿³', 'Ï„Î¿á¿¦Ï„Î¿Î½', 'Î¿á½—Ï„Î¿Î¹', 'Ï„Î¿ÏÏ„Ï‰Î½', 'Ï„Î¿ÏÏ„Î¿Î¹Ï‚', 'Ï„Î¿ÏÏ„Î¿Ï…Ï‚', 
	'Î±á½•Ï„Î·', 'Ï„Î±ÏÏ„Î·Ï‚', 'Ï„Î±ÏÏ„á¿ƒ', 'Ï„Î±ÏÏ„Î·Î½', 'Î±á½•Ï„Î±Î¹', 'Ï„Î±ÏÏ„Î±Î¹Ï‚', 'Ï„Î±ÏÏ„á¾±Ï‚', 'Ï„Î±ÏÏ„Î±Ï‚', 'Ï„Î¿á¿¦Ï„Î¿', 'Ï„Î±á¿¦Ï„Î±', 
	'á¼ÎºÎµá¿–Î½', 'á½…Î´', 'Ï„Î¿á¿¦Î´', 'Ï„á¿·Î´', 'Ï„ÏŒÎ½Î´', 'Î¿á¼µÎ´', 'Ï„á¿¶Î½Î´', 'Ï„Î¿á¿–ÏƒÎ´', 'Ï„Î¿ÏÏƒÎ´', 'á¼¥Î´','Ï„á¿†ÏƒÎ´','Ï„á¿‡Î´','Ï„Î®Î½Î´','Î±á¼µÎ´', 'Ï„Î±á¿–ÏƒÎ´', 
	'Ï„Î¬ÏƒÎ´','Ï„ÏŒÎ´', 'Ï„Î¬Î´'}
	demonstrative_pronouns = demonstrative_pronouns | \
	{normalize('NFD', val) for val in demonstrative_pronouns} | \
	{normalize('NFC', val) for val in demonstrative_pronouns} | \
	{normalize('NFKD', val) for val in demonstrative_pronouns} | \
	{normalize('NFKC', val) for val in demonstrative_pronouns}

	for word in file:
		num_demonstratives += 1 if word in demonstrative_pronouns else 0
		num_characters += len(word)

	return num_demonstratives / num_characters

@textual_feature('sentence_words', 'ancient_greek')
def freq_indefinite_pronoun_in_non_interrogative_sentence(file):
	num_indefinite_pronouns = 0
	num_characters = 0
	interrogative_chars = {';', 'Í¾'}
	pronoun_chars = {'Ï„Î¹Ï‚', 'Ï„Î¹Î½ÏŒÏ‚', 'Ï„Î¹Î½á½¸Ï‚', 'Ï„Î¿Ï…', 'Ï„Î¹Î½Î¯', 'Ï„Î¹Î½á½¶', 'Ï„á¿³', 'Ï„Î¹Î½Î¬', 'Ï„Î¹Î½á½°', 'Ï„Î¹Î½Î­Ï‚', 'Ï„Î¹Î½á½²Ï‚', 'Ï„Î¹Î½á¿¶Î½', 
	'Ï„Î¹ÏƒÎ¯', 'Ï„Î¹Ïƒá½¶', 'Ï„Î¹ÏƒÎ¯Î½', 'Ï„Î¹Ïƒá½¶Î½', 'Ï„Î¹Î½Î¬Ï‚', 'Ï„Î¹Î½á½°Ï‚', 'Ï„Î¹'}
	pronoun_chars = pronoun_chars | \
	{normalize('NFD', val) for val in pronoun_chars} | \
	{normalize('NFC', val) for val in pronoun_chars} | \
	{normalize('NFKD', val) for val in pronoun_chars} | \
	{normalize('NFKC', val) for val in pronoun_chars}

	for line in file:
		if line[-1] not in interrogative_chars and len(line) > 1 and line[-2] not in interrogative_chars:
			for word in line:
				num_indefinite_pronouns += 1 if word in pronoun_chars else 0
				num_characters += len(word)

	return num_indefinite_pronouns / num_characters

# Not different enough from 'freq_indefinite_pronoun_in_non_interrogative_sentence'
# @textual_feature('words', 'ancient_greek')
# def freq_indefinite_pronoun_in_any_sentence(file):
# 	num_indefinite_pronouns = 0
# 	num_characters = 0
# 	pronoun_chars = {'Ï„Î¹Ï‚', 'Ï„Î¹Î½ÏŒÏ‚', 'Ï„Î¹Î½á½¸Ï‚', 'Ï„Î¿Ï…', 'Ï„Î¹Î½Î¯', 'Ï„Î¹Î½á½¶', 'Ï„á¿³', 'Ï„Î¹Î½Î¬', 'Ï„Î¹Î½á½°', 'Ï„Î¹Î½Î­Ï‚', 'Ï„Î¹Î½á½²Ï‚', 'Ï„Î¹Î½á¿¶Î½', 
# 	'Ï„Î¹ÏƒÎ¯', 'Ï„Î¹Ïƒá½¶', 'Ï„Î¹ÏƒÎ¯Î½', 'Ï„Î¹Ïƒá½¶Î½', 'Ï„Î¹Î½Î¬Ï‚', 'Ï„Î¹Î½á½°Ï‚', 'Ï„Î¹'}
# 	pronoun_chars = pronoun_chars | \
# 	{normalize('NFD', val) for val in pronoun_chars} | \
# 	{normalize('NFC', val) for val in pronoun_chars} | \
# 	{normalize('NFKD', val) for val in pronoun_chars} | \
# 	{normalize('NFKC', val) for val in pronoun_chars}

# 	for word in file:
# 		num_indefinite_pronouns += 1 if word in pronoun_chars else 0
# 		num_characters += len(word)

# 	return num_indefinite_pronouns / num_characters

@textual_feature('words', 'ancient_greek')
def freq_allos(file):
	num_allos = 0
	num_characters = 0
	allos_characters = {'á¼„Î»Î»Î¿Ï‚', 'á¼„Î»Î»Î·', 'á¼„Î»Î»Î¿', 'á¼„Î»Î»Î¿Ï…', 'á¼„Î»Î»á¿³', 'á¼„Î»Î»Î¿Î½', 'á¼„Î»Î»Î¿Î¹', 'á¼„Î»Î»Ï‰Î½', 'á¼„Î»Î»Î¿Î¹Ï‚', 'á¼„Î»Î»Î¿Ï…Ï‚', 
	'á¼„Î»Î»Î·Ï‚', 'á¼„Î»Î»á¿ƒ', 'á¼„Î»Î»Î·Î½', 'á¼„Î»Î»Î±Î¹', 'á¼„Î»Î»á¾±Ï‚', 'á¼„Î»Î»Î±Ï‚', 'á¼„Î»Î»Î±'}
	allos_characters = allos_characters | \
	{normalize('NFD', val) for val in allos_characters} | \
	{normalize('NFC', val) for val in allos_characters} | \
	{normalize('NFKD', val) for val in allos_characters} | \
	{normalize('NFKC', val) for val in allos_characters}

	for word in file:
		num_allos += 1 if word in allos_characters else 0
		num_characters += len(word)

	return num_allos / num_characters

@textual_feature('words', 'ancient_greek')
def freq_autos(file):
	num_autos = 0
	num_characters = 0
	autos_characters = {'Î±á½Ï„ÏŒÏ‚', 'Î±á½Ï„á½¸Ï‚', 'Î±á½Ï„Î¿á¿¦', 'Î±á½Ï„á¿·', 'Î±á½Ï„ÏŒÎ½', 'Î±á½Ï„á½¸Î½', 'Î±á½Ï„Î¿Î¯', 'Î±á½Ï„Î¿á½¶', 'Î±á½Ï„á¿¶Î½', 'Î±á½Ï„Î¿á¿–Ï‚', 
	'Î±á½Ï„Î¿ÏÏ‚', 'Î±á½Ï„Î¿á½ºÏ‚', 'Î±á½Ï„Î®', 'Î±á½Ï„á½´', 'Î±á½Ï„á¿†Ï‚', 'Î±á½Ï„á¿‡', 'Î±á½Ï„Î®Î½', 'Î±á½Ï„á½´Î½', 'Î±á½Ï„Î±Î¯', 'Î±á½Ï„Î±á½¶', 'Î±á½Ï„Î±á¿–Ï‚', 'Î±á½Ï„á¾±Ï‚', 
	'Î±á½Ï„á¾±ÌÏ‚', 'Î±á½Ï„Î¬Ï‚', 'Î±á½Ï„á½°Ï‚', 'Î±á½Ï„ÏŒ', 'Î±á½Ï„á½¸', 'Î±á½Ï„Î¬', 'Î±á½Ï„á½°'}
	autos_characters = autos_characters | \
	{normalize('NFD', val) for val in autos_characters} | \
	{normalize('NFC', val) for val in autos_characters} | \
	{normalize('NFKD', val) for val in autos_characters} | \
	{normalize('NFKC', val) for val in autos_characters}

	for word in file:
		num_autos += 1 if word in autos_characters else 0
		num_characters += len(word)

	return num_autos / num_characters

@textual_feature('words', 'ancient_greek')
def freq_reflexive(file):
	num_reflexive = 0
	num_characters = 0

	reflexive_characters = {'á¼Î¼Î±Ï…Ï„Î¿á¿¦', 'á¼Î¼Î±Ï…Ï„á¿·', 'á¼Î¼Î±Ï…Ï„ÏŒÎ½', 'á¼Î¼Î±Ï…Ï„á½¸Î½', 'á¼Î¼Î±Ï…Ï„á¿†Ï‚', 'á¼Î¼Î±Ï…Ï„á¿‡', 'á¼Î¼Î±Ï…Ï„Î®Î½', 'á¼Î¼Î±Ï…Ï„á½´Î½', 
	'ÏƒÎµÎ±Ï…Ï„Î¿á¿¦', 'ÏƒÎµÎ±Ï…Ï„á¿·', 'ÏƒÎµÎ±Ï…Ï„ÏŒÎ½', 'ÏƒÎµÎ±Ï…Ï„á½¸Î½', 'ÏƒÎµÎ±Ï…Ï„á¿†Ï‚', 'ÏƒÎµÎ±Ï…Ï„á¿‡', 'ÏƒÎµÎ±Ï…Ï„Î®Î½', 'ÏƒÎµÎ±Ï…Ï„á½´Î½', 'á¼‘Î±Ï…Ï„Î¿á¿¦', 'á¼‘Î±Ï…Ï„á¿·', 'á¼‘Î±Ï…Ï„ÏŒÎ½', 
	'á¼‘Î±Ï…Ï„á½¸Î½', 'á¼‘Î±Ï…Ï„á¿¶Î½', 'á¼‘Î±Ï…Ï„Î¿á¿–Ï‚', 'á¼‘Î±Ï…Ï„Î¿ÏÏ‚', 'á¼‘Î±Ï…Ï„Î¿á½ºÏ‚', 'á¼‘Î±Ï…Ï„á¿†Ï‚', 'á¼‘Î±Ï…Ï„á¿‡', 'á¼‘Î±Ï…Ï„Î®Î½', 'á¼‘Î±Ï…Ï„á½´Î½', 'á¼‘Î±Ï…Ï„Î±á¿–Ï‚', 'á¼‘Î±Ï…Ï„Î¬Ï‚', 
	'á¼‘Î±Ï…Ï„á½°Ï‚', 'á¼‘Î±Ï…Ï„ÏŒ', 'á¼‘Î±Ï…Ï„á½¸', 'á¼‘Î±Ï…Ï„Î¬', 'á¼‘Î±Ï…Ï„á½°'}
	reflexive_characters = reflexive_characters | \
	{normalize('NFD', val) for val in reflexive_characters} | \
	{normalize('NFC', val) for val in reflexive_characters} | \
	{normalize('NFKD', val) for val in reflexive_characters} | \
	{normalize('NFKC', val) for val in reflexive_characters}

	bigram_reflexive_characters = {'á¼¡Î¼á¿¶Î½': {'Î±á½Ï„á¿¶Î½'}, 'á¼¡Î¼á¿–Î½': {'Î±á½Ï„Î¿á¿–Ï‚', 'Î±á½Ï„Î±á¿–Ï‚'}, 
	'á¼¡Î¼á¾¶Ï‚': {'Î±á½Ï„Î¿ÏÏ‚', 'Î±á½Ï„Î¿á½ºÏ‚', 'Î±á½Ï„Î¬Ï‚', 'Î±á½Ï„á½°Ï‚'}, 'á½‘Î¼á¿¶Î½': {'Î±á½Ï„á¿¶Î½'}, 'á½‘Î¼á¿–Î½': {'Î±á½Ï„Î¿á¿–Ï‚', 'Î±á½Ï„Î±á¿–Ï‚'}, 
	'á½‘Î¼á¾¶Ï‚': {'Î±á½Ï„Î¿ÏÏ‚', 'Î±á½Ï„Î¿á½ºÏ‚', 'Î±á½Ï„Î¬Ï‚', 'Î±á½Ï„á½°Ï‚'}, 'ÏƒÏ†á¿¶Î½': {'Î±á½Ï„á¿¶Î½'}, 'ÏƒÏ†Î¯ÏƒÎ¹Î½': {'Î±á½Ï„Î¿á¿–Ï‚', 'Î±á½Ï„Î±á¿–Ï‚'}, 
	'ÏƒÏ†á¾¶Ï‚': {'Î±á½Ï„Î¿ÏÏ‚', 'Î±á½Ï„Î¿á½ºÏ‚', 'Î±á½Ï„Î¬Ï‚', 'Î±á½Ï„á½°Ï‚'}}
	#This is just verbose syntax for normalizing all the keys and values in the dictionary with NFD, NFC, NFKD, & NFKC
	#The double star (**) unpacking is how dictionaries are merged https://stackoverflow.com/a/26853961/7102572
	bigram_reflexive_characters = {**bigram_reflexive_characters, 
	**{normalize('NFD', key): {normalize('NFD', v) for v in val} for key, val in bigram_reflexive_characters.items()}, 
	**{normalize('NFC', key): {normalize('NFC', v) for v in val} for key, val in bigram_reflexive_characters.items()}, 
	**{normalize('NFKD', key): {normalize('NFKD', v) for v in val} for key, val in bigram_reflexive_characters.items()}, 
	**{normalize('NFKC', key): {normalize('NFKC', v) for v in val} for key, val in bigram_reflexive_characters.items()}}

	bigram_first_half = None
	for word in file:

		#Found monogram characters
		if word in reflexive_characters:
			num_reflexive += 1
			bigram_first_half = None

		#Found the first part of the reflexive bigram
		elif word in bigram_reflexive_characters:
			bigram_first_half = word

		#Found the second part of the reflexive bigram
		elif bigram_first_half in bigram_reflexive_characters and word in bigram_reflexive_characters[bigram_first_half]:
			num_reflexive += 2
			bigram_first_half = None

		#Default case
		else:
			bigram_first_half = None

		num_characters += len(word)

	return num_reflexive / num_characters

@textual_feature('sentence_words', 'ancient_greek')
def freq_sentences_with_vocative_omega(file):
	num_vocatives = 0
	vocative_characters = {'á½¦'}
	vocative_characters = vocative_characters | \
	{normalize('NFD', val) for val in vocative_characters} | \
	{normalize('NFC', val) for val in vocative_characters} | \
	{normalize('NFKD', val) for val in vocative_characters} | \
	{normalize('NFKC', val) for val in vocative_characters}

	for line in file:
		for word in line:
			if word in vocative_characters:
				num_vocatives += 1
				break

	return num_vocatives / len(file)

@textual_feature('words', 'ancient_greek')
def freq_superlative(file):
	num_superlative = 0
	num_characters = 0
	superlative_ending_characters = ['Ï„Î±Ï„Î¿Ï‚', 'Ï„Î¬Ï„Î¿Ï…', 'Ï„Î¬Ï„á¿³', 'Ï„Î±Ï„Î¿Î½', 'Ï„Î±Ï„Î¿Î¹', 'Ï„Î¬Ï„Ï‰Î½', 
	'Ï„Î¬Ï„Î¿Î¹Ï‚', 'Ï„Î¬Ï„Î¿Ï…Ï‚', 'Ï„Î¬Ï„Î·', 'Ï„Î¬Ï„Î·Ï‚', 'Ï„Î¬Ï„á¿ƒ', 'Ï„Î¬Ï„Î·Î½', 
	'Ï„Î¬Ï„Î±Î¹Ï‚', 'Ï„Î¬Ï„Î±Ï‚', 'Ï„Î±Ï„Î±','Ï„Î±Ï„Î¬', 'Ï„Î±Ï„Îµ']
	#The endswith() method requires a tuple
	superlative_ending_characters = tuple(superlative_ending_characters + \
	[normalize('NFD', val) for val in superlative_ending_characters] + \
	[normalize('NFC', val) for val in superlative_ending_characters] + \
	[normalize('NFKD', val) for val in superlative_ending_characters] + \
	[normalize('NFKC', val) for val in superlative_ending_characters])

	for word in file:
		num_superlative += 1 if word.endswith(superlative_ending_characters) else 0
		num_characters += len(word)

	return num_superlative / num_characters

@textual_feature('words', 'ancient_greek')
def freq_conjunction(file):
	num_conjunction = 0
	num_characters = 0
	conjunction_chars = {'Ï„Îµ', 'ÎºÎ±Î¯', 'ÎºÎ±á½¶', 'á¼€Î»Î»Î¬', 'á¼€Î»Î»á½°', 'ÎºÎ±Î¯Ï„Î¿Î¹', 'Î¿á½Î´Î­', 'Î¿á½Î´á½²', 'Î¼Î·Î´Î­', 'Î¼Î·Î´á½²', 'Î¿á½”Ï„Îµ', 'Î¿á½”Ï„', 'Î¼Î®Ï„Îµ', 'Î¼Î®Ï„', 'Î¿á½Î´', 'Î¼Î·Î´', 'á¼¤', 'á¼¢', 'Ï„'}
	conjunction_chars = conjunction_chars | \
	{normalize('NFD', val) for val in conjunction_chars} | \
	{normalize('NFC', val) for val in conjunction_chars} | \
	{normalize('NFKD', val) for val in conjunction_chars} | \
	{normalize('NFKC', val) for val in conjunction_chars}

	for word in file:
		num_conjunction += 1 if word in conjunction_chars else 0
		num_characters += len(word)

	return num_conjunction / num_characters

@textual_feature('sentence_words', 'ancient_greek')
def mean_sentence_length(file):
	return reduce(lambda cur_len, line: cur_len + 
		reduce(lambda word_len, word: word_len + len(word), line, 0), file, 0) / len(file)

@textual_feature('sentence_words', 'ancient_greek')
def freq_sentence_with_relative_clause(file):
	num_sentence_with_clause = 0
	num_sentences = 0
	pronouns = {'á½…Ï‚', 'á½ƒÏ‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'á½ƒÎ½', 'Î¿á¼µ', 'Î¿á¼³', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'Î¿á½“Ï‚', 'á¼¥', 'á¼£', 'á¼§Ï‚', 'á¾—', 
	'á¼¥Î½', 'á¼£Î½', 'Î±á¼µ', 'Î±á¼³', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á¼ƒÏ‚', 'á½…', 'á½ƒ', 'á¼…', 'á¼ƒ'}
	pronouns = pronouns | \
	{normalize('NFD', val) for val in pronouns} | \
	{normalize('NFC', val) for val in pronouns} | \
	{normalize('NFKD', val) for val in pronouns} | \
	{normalize('NFKC', val) for val in pronouns}

	for line in file:
		for word in line:
			if word in pronouns:
				num_sentence_with_clause += 1
				break
		num_sentences += 1

	return num_sentence_with_clause / num_sentences

@textual_feature('words', 'ancient_greek')
def mean_length_relative_clause(file):
	num_relative_clause = 0
	sum_length_relative_clause = 0
	pronouns = {'á½…Ï‚', 'á½ƒÏ‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'á½ƒÎ½', 'Î¿á¼µ', 'Î¿á¼³', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'Î¿á½“Ï‚', 'á¼¥', 'á¼£', 'á¼§Ï‚', 'á¾—', 
	'á¼¥Î½', 'á¼£Î½', 'Î±á¼µ', 'Î±á¼³', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á¼ƒÏ‚', 'á½…', 'á½ƒ', 'á¼…', 'á¼ƒ'}
	pronouns = pronouns | \
	{normalize('NFD', val) for val in pronouns} | \
	{normalize('NFC', val) for val in pronouns} | \
	{normalize('NFKD', val) for val in pronouns} | \
	{normalize('NFKC', val) for val in pronouns}
	punctuation = {'.', ',', ':', ';', 'Í¾'}
	punctuation = punctuation | \
	{normalize('NFD', val) for val in punctuation} | \
	{normalize('NFC', val) for val in punctuation} | \
	{normalize('NFKD', val) for val in punctuation} | \
	{normalize('NFKC', val) for val in punctuation}

	in_relative_clause = False

	for word in file:
		if word in punctuation:
			in_relative_clause = False
		elif word in pronouns:
			in_relative_clause = True
			num_relative_clause += 1
		if in_relative_clause:
			sum_length_relative_clause += len(word)

	return 0 if num_relative_clause == 0 else sum_length_relative_clause / num_relative_clause

# Too similar to freq_sentence_with_relative_clause
# @textual_feature('sentence_words', 'ancient_greek')
# def relative_clause_per_non_interrogative_sentence(file):
# 	num_relative_pronoun = 0
# 	num_non_interrogative_sentence = 0
# 	interrogative_chars = {';', 'Í¾'} #Second character is Greek semi colon
# 	pronouns = {'á½…Ï‚', 'á½ƒÏ‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'á½ƒÎ½', 'Î¿á¼µ', 'Î¿á¼³', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'Î¿á½“Ï‚', 'á¼¥', 'á¼£', 'á¼§Ï‚', 'á¾—', 
# 	'á¼¥Î½', 'á¼£Î½', 'Î±á¼µ', 'Î±á¼³', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á¼ƒÏ‚', 'á½…', 'á½ƒ', 'á¼…', 'á¼ƒ'}
# 	pronouns = pronouns | \
# 	{normalize('NFD', val) for val in pronouns} | \
# 	{normalize('NFC', val) for val in pronouns} | \
# 	{normalize('NFKD', val) for val in pronouns} | \
# 	{normalize('NFKC', val) for val in pronouns}

# 	for line in file:
# 		if line[-1] not in interrogative_chars and len(line) > 1 and line[-2] not in interrogative_chars:
# 			for word in line:
# 				num_relative_pronoun += 1 if word in pronouns else 0
# 			num_non_interrogative_sentence += 1

# 	return num_relative_pronoun / num_non_interrogative_sentence

@textual_feature('words', 'ancient_greek')
def freq_circumstantial_markers(file):
	num_participles = 0
	num_characters = 0
	participles = {'á¼”Ï€ÎµÎ¹Ï„Î±', 'á½…Î¼Ï‰Ï‚', 'ÎºÎ±Î¯Ï€ÎµÏ', 'á¼…Ï„Îµ', 'á¼”Ï€ÎµÎ¹Ï„', 'á¼…Ï„', 'á½Î¼á¿¶Ï‚'}
	participles = participles | \
	{normalize('NFD', val) for val in participles} | \
	{normalize('NFC', val) for val in participles} | \
	{normalize('NFKD', val) for val in participles} | \
	{normalize('NFKC', val) for val in participles}

	for word in file:
		num_participles += 1 if word in participles else 0
		num_characters += len(word)

	return num_participles / num_characters

@textual_feature('words', 'ancient_greek')
def freq_hina(file):
	num_hina = 0
	num_characters = 0
	ina_characters = {'á¼µÎ½Î±', 'á¼µÎ½'}
	ina_characters = ina_characters | \
	{normalize('NFD', val) for val in ina_characters} | \
	{normalize('NFC', val) for val in ina_characters} | \
	{normalize('NFKD', val) for val in ina_characters} | \
	{normalize('NFKC', val) for val in ina_characters}

	for word in file:
		num_hina += 1 if word in ina_characters else 0
		num_characters += len(word)

	return num_hina / num_characters

@textual_feature('words', 'ancient_greek')
def freq_hopos(file):
	num_hopos = 0
	num_characters = 0
	hopos_characters = {'á½…Ï€Ï‰Ï‚'}
	hopos_characters = hopos_characters | \
	{normalize('NFD', val) for val in hopos_characters} | \
	{normalize('NFC', val) for val in hopos_characters} | \
	{normalize('NFKD', val) for val in hopos_characters} | \
	{normalize('NFKC', val) for val in hopos_characters}

	for word in file:
		num_hopos += 1 if word in hopos_characters else 0
		num_characters += len(word)

	return num_hopos / num_characters

@textual_feature('words', 'ancient_greek')
def freq_ws(file):
	num_ws = 0
	num_characters = 0
	ws_characters = {'á½¡Ï‚'}
	ws_characters = ws_characters | \
	{normalize('NFD', val) for val in ws_characters} | \
	{normalize('NFC', val) for val in ws_characters} | \
	{normalize('NFKD', val) for val in ws_characters} | \
	{normalize('NFKC', val) for val in ws_characters}

	for word in file:
		num_ws += 1 if word in ws_characters else 0
		num_characters += len(word)

	return num_ws / num_characters

# Bad feature when result is NaN or infinity
# @textual_feature('words', 'ancient_greek')
# def ratio_ina_to_opos(file):
# 	num_ina = 0
# 	num_opos = 0
# 	ina_chars = {'á¼µÎ½Î±'}
# 	ina_chars = ina_chars | \
# 	{normalize('NFD', val) for val in ina_chars} | \
# 	{normalize('NFC', val) for val in ina_chars} | \
# 	{normalize('NFKD', val) for val in ina_chars} | \
# 	{normalize('NFKC', val) for val in ina_chars}
# 	opos_chars = {'á½…Ï€Ï‰Ï‚'}
# 	opos_chars = opos_chars | \
# 	{normalize('NFD', val) for val in opos_chars} | \
# 	{normalize('NFC', val) for val in opos_chars} | \
# 	{normalize('NFKD', val) for val in opos_chars} | \
# 	{normalize('NFKC', val) for val in opos_chars}

# 	for word in file:
# 		if word in ina_chars:
# 			num_ina += 1
# 		elif word in opos_chars:
# 			num_opos += 1

# 	return math.nan if num_ina == 0 and num_opos == 0 else math.inf if num_opos == 0 else num_ina / num_opos

@textual_feature('words', 'ancient_greek')
def freq_wste_not_preceded_by_eta(file):
	num_wste = 0
	num_characters = 0
	wste_characters = {'á½¥ÏƒÏ„Îµ'}
	wste_characters = wste_characters | \
	{normalize('NFD', val) for val in wste_characters} | \
	{normalize('NFC', val) for val in wste_characters} | \
	{normalize('NFKD', val) for val in wste_characters} | \
	{normalize('NFKC', val) for val in wste_characters}
	eta_chars = {'á¼¤', 'á¼¢'}
	eta_chars = eta_chars | \
	{normalize('NFD', val) for val in eta_chars} | \
	{normalize('NFC', val) for val in eta_chars} | \
	{normalize('NFKD', val) for val in eta_chars} | \
	{normalize('NFKC', val) for val in eta_chars}
	ok_to_add = True

	for word in file:
		num_wste += 1 if word in wste_characters and ok_to_add else 0
		num_characters += len(word)
		ok_to_add = word not in eta_chars

	return num_wste / num_characters

# Only 54 matches across 42 files for regex "(Î·Ì“Ì|Î·Ì“Ì€) (\w+ )*?Ï‰Ì”ÌÏƒÏ„Îµ" and 33 matches across 27 files for "(Î·Ì“Ì|Î·Ì“Ì€) Ï‰Ì”ÌÏƒÏ„Îµ"
# @textual_feature('words', 'ancient_greek')
# def freq_wste_preceded_by_eta(file):
# 	num_wste_characters = 0
# 	num_characters = 0
# 	wste_characters = {'á½¥ÏƒÏ„Îµ'}
# 	wste_characters = wste_characters | \
# 	{normalize('NFD', val) for val in wste_characters} | \
# 	{normalize('NFC', val) for val in wste_characters} | \
# 	{normalize('NFKD', val) for val in wste_characters} | \
# 	{normalize('NFKC', val) for val in wste_characters}
# 	eta_chars = {'á¼¤', 'á¼¢'}
# 	eta_chars = eta_chars | \
# 	{normalize('NFD', val) for val in eta_chars} | \
# 	{normalize('NFC', val) for val in eta_chars} | \
# 	{normalize('NFKD', val) for val in eta_chars} | \
# 	{normalize('NFKC', val) for val in eta_chars}
# 	ok_to_add = False

# 	for word in file:
# 		num_wste_characters += len(word) if word in wste_characters and ok_to_add else 0
# 		num_characters += len(word)
# 		ok_to_add = word in eta_chars

# 	return num_wste_characters / num_characters

@textual_feature('words', 'ancient_greek')
def freq_temporal_causal_markers(file):
	num_clause_words = 0
	num_characters = 0
	clause_chars = {'Î¼Î­Ï°ÏÎ¹', 'á¼•Ï‰Ï‚', 'Ï€ÏÎ¯Î½', 'Ï€Ïá½¶Î½', 'á¼Ï€ÎµÎ¯', 'á¼Ï€Îµá½¶', 'á¼Ï€ÎµÎ¹Î´Î®', 'á¼Ï€ÎµÎ¹Î´á½´', 'á¼Ï€ÎµÎ¹Î´Î¬Î½', 'á¼Ï€ÎµÎ¹Î´á½°Î½', 'á½…Ï„Îµ', 'á½…Ï„Î±Î½'}
	clause_chars = clause_chars | \
	{normalize('NFD', val) for val in clause_chars} | \
	{normalize('NFC', val) for val in clause_chars} | \
	{normalize('NFKD', val) for val in clause_chars} | \
	{normalize('NFKC', val) for val in clause_chars}

	for word in file:
		num_clause_words += 1 if word in clause_chars else 0
		num_characters += len(word)

	return num_clause_words / num_characters

@textual_feature('sentence_words', 'ancient_greek')
def variance_of_sentence_length(file):
	num_sentences = 0
	total_len = 0

	for line in file:
		num_sentences += 1
		total_len += reduce(lambda cur_len, word: cur_len + len(word), line, 0)
	mean = total_len / num_sentences
	squared_difference = 0
	for line in file:
		squared_difference += (reduce(lambda cur_len, word: cur_len + len(word), line, 0) - mean) ** 2

	return squared_difference / num_sentences

@textual_feature('words', 'ancient_greek')
def freq_particles(file):
	num_particles = 0
	num_characters = 0
	#Word tokenizer doesn't work well with ellision - apostrophes are removed
	particles = {'á¼„Î½', 'á¼‚Î½', 'á¼†ÏÎ±', 'Î³Îµ', "Î³", "Î´", 'Î´Î­', 'Î´á½²', 'Î´Î®', 'Î´á½´', 'á¼•Ï‰Ï‚', "Îº", 'ÎºÎµ', 'ÎºÎ­', 'Îºá½²', 'ÎºÎ­Î½', 'Îºá½²Î½', 
	'ÎºÎµÎ½', 'Î¼Î¬', 'Î¼á½°' 'Î¼Î­Î½', 'Î¼á½²Î½', 'Î¼Î­Î½Ï„Î¿Î¹', 'Î¼Î®Î½', 'Î¼á½´Î½', 'Î¼á¿¶Î½', 'Î½Ï', 'Î½á½º', 'Î½Ï…', 'Î¿á½–Î½', 
	'Ï€ÎµÏ', 'Ï€Ï‰', 'Ï„Î¿Î¹'}
	particles = particles | \
	{normalize('NFD', val) for val in particles} | \
	{normalize('NFC', val) for val in particles} | \
	{normalize('NFKD', val) for val in particles} | \
	{normalize('NFKC', val) for val in particles}

	for word in file:
		num_particles += 1 if word in particles else 0
		num_characters += len(word)

	return num_particles / num_characters

# No interpunct symbols found in the entire tesserae corpus - searched with regex: (Â·|Î‡|âˆ™|â‹…|â€¢|á›«|â€§|â¦|â¸³|ãƒ»|ê|ï½¥|ğ„)
# @textual_feature('default', 'ancient_greek')
# def freq_raised_dot(file):
# 	#Unicode from https://en.wikipedia.org/wiki/Interpunct#Similar_symbols
# 	#'\u00B7' is 'Â·', '\u0387' is 'Î‡', '\u2219' is 'âˆ™', '\u22C5' is 'â‹…', '\u2022' is 'â€¢', '\u16EB' is 'á›«', '\u2027' is 'â€§', 
# 	#'\u2981' is 'â¦', '\u2E33' is 'â¸³', '\u30FB' is 'ãƒ»', '\uA78F' is 'ê', '\uFF65' is 'ï½¥', '\U00010101' is 'ğ„'
# 	dot_chars = {'Â·', 'Î‡', 'âˆ™', 'â‹…', 'â€¢', 'á›«', 'â€§', 'â¦', 'â¸³', 'ãƒ»', 'ê', 'ï½¥', 'ğ„'}
# 	num_dot_chars = 0
# 	for char in file:
# 		num_dot_chars += 1 if char in dot_chars else 0
# 	return num_dot_chars / len(file)

@textual_feature('words', 'ancient_greek')
def freq_men(file):
	num_men = 0
	num_characters = 0
	men_chars = {'Î¼Î­Î½', 'Î¼á½²Î½'}
	men_chars = men_chars | \
	{normalize('NFD', val) for val in men_chars} | \
	{normalize('NFC', val) for val in men_chars} | \
	{normalize('NFKD', val) for val in men_chars} | \
	{normalize('NFKC', val) for val in men_chars}

	for word in file:
		num_men += 1 if word in men_chars else 0
		num_characters += len(word)
	return num_men / num_characters
